#!/usr/bin/env python3
"""
Improved Speech Segmentation Module for Enhanced FastWhisper Server
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ç–æ—á–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞
"""

import logging
import numpy as np
import time
from typing import Dict, List, Optional, Tuple, Any
from collections import deque
from dataclasses import dataclass
from enum import Enum
import asyncio
from datetime import datetime

logger = logging.getLogger(__name__)

class SpeechState(Enum):
    """–°–æ—Å—Ç–æ—è–Ω–∏—è —Ä–µ—á–µ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
    SILENCE = "silence"
    SPEECH_DETECTION = "speech_detection"
    SPEECH_ACTIVE = "speech_active"
    SPEECH_ENDING = "speech_ending"
    COMMAND_COMPLETE = "command_complete"

@dataclass
class AudioSegment:
    """–ê—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    audio_data: np.ndarray
    start_time: float
    end_time: float
    confidence: float
    state: SpeechState
    vad_scores: List[float]
    energy_level: float

class ImprovedClientBuffer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –±—É—Ñ–µ—Ä –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ —Å —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π"""
    
    def __init__(self, client_id: str, config: Dict):
        self.client_id = client_id
        self.config = config
        
        # –ê—É–¥–∏–æ –±—É—Ñ–µ—Ä—ã
        self.audio_buffer = np.array([])
        self.vad_scores = deque(maxlen=100)
        self.energy_history = deque(maxlen=50)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.current_state = SpeechState.SILENCE
        self.speech_start_time = None
        self.last_speech_time = None
        self.silence_counter = 0
        self.speech_counter = 0
        
        # –ü–æ—Ä–æ–≥–∏ –∏ —Å—á–µ—Ç—á–∏–∫–∏
        self.speech_threshold = config.get('segmentation_speech_threshold', 0.35)
        self.silence_threshold = config.get('segmentation_silence_threshold', 0.25)
        self.min_command_duration = config.get('min_command_duration', 0.8)
        self.max_command_duration = config.get('max_command_duration', 20.0)
        self.speech_confirmation_chunks = config.get('speech_confirmation_chunks', 3)
        self.silence_confirmation_chunks = config.get('silence_confirmation_chunks', 8)
        
        # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏
        self.energy_threshold = 0.001
        self.background_noise = 0.0002
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'commands_segmented': 0,
            'false_starts': 0,
            'successful_commands': 0,
            'total_chunks': 0,
            'average_command_duration': 0.0
        }
        
        logger.debug(f"üéØ Created buffer for client {client_id}")
    
    def process_chunk(self, audio_chunk: np.ndarray, vad_score: float) -> Optional[np.ndarray]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ —á–∞–Ω–∫–∞ —Å —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥—ã –∫–æ–≥–¥–∞ –æ–Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
        """
        self.stats['total_chunks'] += 1
        chunk_time = time.time()
        
        # –†–∞—Å—á–µ—Ç —ç–Ω–µ—Ä–≥–∏–∏ —á–∞–Ω–∫–∞
        rms_energy = np.sqrt(np.mean(audio_chunk ** 2))
        self.energy_history.append(rms_energy)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è VAD score
        normalized_vad = self._normalize_vad_score(vad_score, rms_energy)
        self.vad_scores.append(normalized_vad)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –º–∞—à–∏–Ω—ã –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        completed_segment = self._update_state_machine(
            audio_chunk, normalized_vad, chunk_time
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –∫ –±—É—Ñ–µ—Ä—É –µ—Å–ª–∏ –º—ã –≤ —Ä–µ–∂–∏–º–µ –∑–∞–ø–∏—Å–∏
        if self.current_state in [SpeechState.SPEECH_DETECTION, 
                                  SpeechState.SPEECH_ACTIVE, 
                                  SpeechState.SPEECH_ENDING]:
            self.audio_buffer = np.concatenate([self.audio_buffer, audio_chunk])
        
        return completed_segment
    
    def _normalize_vad_score(self, vad_score: float, energy: float) -> float:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è VAD score —Å —É—á–µ—Ç–æ–º —ç–Ω–µ—Ä–≥–∏–∏"""
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ —à—É–º–∞
        if len(self.energy_history) > 10:
            avg_energy = np.mean(list(self.energy_history)[-10:])
            if avg_energy < self.energy_threshold:
                self.background_noise = avg_energy * 0.9
        
        # –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –±—É—Å—Ç –¥–ª—è —Å–ª–∞–±—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
        if energy > self.background_noise * 3:
            energy_boost = min(energy / self.energy_threshold, 2.0)
            vad_score = min(vad_score * energy_boost, 1.0)
        
        # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ VAD score
        if len(self.vad_scores) >= 3:
            recent_scores = list(self.vad_scores)[-3:]
            smoothed = np.mean(recent_scores + [vad_score])
            return max(0.0, min(1.0, smoothed))
        
        return vad_score
    
    def _update_state_machine(self, audio_chunk: np.ndarray, vad_score: float, 
                            chunk_time: float) -> Optional[np.ndarray]:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞—à–∏–Ω—ã —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        
        previous_state = self.current_state
        
        if self.current_state == SpeechState.SILENCE:
            return self._handle_silence_state(audio_chunk, vad_score, chunk_time)
        
        elif self.current_state == SpeechState.SPEECH_DETECTION:
            return self._handle_detection_state(audio_chunk, vad_score, chunk_time)
        
        elif self.current_state == SpeechState.SPEECH_ACTIVE:
            return self._handle_active_state(audio_chunk, vad_score, chunk_time)
        
        elif self.current_state == SpeechState.SPEECH_ENDING:
            return self._handle_ending_state(audio_chunk, vad_score, chunk_time)
        
        return None
    
    def _handle_silence_state(self, audio_chunk: np.ndarray, vad_score: float, 
                            chunk_time: float) -> Optional[np.ndarray]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ç–∏—à–∏–Ω—ã"""
        
        if vad_score > self.speech_threshold:
            self.speech_counter += 1
            self.silence_counter = 0
            
            if self.speech_counter >= self.speech_confirmation_chunks:
                # –ù–∞—á–∞–ª–æ —Ä–µ—á–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ
                self.current_state = SpeechState.SPEECH_DETECTION
                self.speech_start_time = chunk_time
                self.last_speech_time = chunk_time
                self.audio_buffer = audio_chunk.copy()
                
                logger.debug(f"üéØ Speech detection started for {self.client_id}")
                return None
        else:
            self.speech_counter = max(0, self.speech_counter - 1)
            self.silence_counter += 1
        
        return None
    
    def _handle_detection_state(self, audio_chunk: np.ndarray, vad_score: float, 
                              chunk_time: float) -> Optional[np.ndarray]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏"""
        
        if vad_score > self.speech_threshold:
            self.speech_counter += 1
            self.silence_counter = 0
            self.last_speech_time = chunk_time
            
            # –ü–µ—Ä–µ—Ö–æ–¥ –∫ –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–µ—á–∏
            if self.speech_counter >= self.speech_confirmation_chunks * 2:
                self.current_state = SpeechState.SPEECH_ACTIVE
                logger.debug(f"üéØ Speech active for {self.client_id}")
        
        elif vad_score < self.silence_threshold:
            self.silence_counter += 1
            self.speech_counter = max(0, self.speech_counter - 1)
            
            # –õ–æ–∂–Ω—ã–π —Å—Ç–∞—Ä—Ç - –≤–æ–∑–≤—Ä–∞—Ç –∫ —Ç–∏—à–∏–Ω–µ
            if self.silence_counter >= self.silence_confirmation_chunks:
                duration = chunk_time - self.speech_start_time if self.speech_start_time else 0
                
                if duration < self.min_command_duration:
                    logger.debug(f"üéØ False start detected for {self.client_id} ({duration:.1f}s)")
                    self.stats['false_starts'] += 1
                    self._reset_to_silence()
                else:
                    # –ö–æ—Ä–æ—Ç–∫–∞—è –∫–æ–º–∞–Ω–¥–∞ - –∑–∞–≤–µ—Ä—à–∞–µ–º
                    return self._complete_command()
        
        return None
    
    def _handle_active_state(self, audio_chunk: np.ndarray, vad_score: float, 
                           chunk_time: float) -> Optional[np.ndarray]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ–π —Ä–µ—á–∏"""
        
        current_duration = chunk_time - self.speech_start_time if self.speech_start_time else 0
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if current_duration > self.max_command_duration:
            logger.warning(f"üéØ Command too long for {self.client_id} ({current_duration:.1f}s), forcing completion")
            return self._complete_command()
        
        if vad_score > self.speech_threshold:
            self.speech_counter += 1
            self.silence_counter = 0
            self.last_speech_time = chunk_time
        
        elif vad_score < self.silence_threshold:
            self.silence_counter += 1
            self.speech_counter = max(0, self.speech_counter - 1)
            
            # –ù–∞—á–∞–ª–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã
            if self.silence_counter >= self.silence_confirmation_chunks // 2:
                self.current_state = SpeechState.SPEECH_ENDING
                logger.debug(f"üéØ Speech ending for {self.client_id}")
        
        return None
    
    def _handle_ending_state(self, audio_chunk: np.ndarray, vad_score: float, 
                           chunk_time: float) -> Optional[np.ndarray]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–µ—á–∏"""
        
        if vad_score > self.speech_threshold:
            # –†–µ—á—å –≤–æ–∑–æ–±–Ω–æ–≤–∏–ª–∞—Å—å - –≤–æ–∑–≤—Ä–∞—Ç –∫ –∞–∫—Ç–∏–≤–Ω–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
            self.current_state = SpeechState.SPEECH_ACTIVE
            self.speech_counter += 1
            self.silence_counter = 0
            self.last_speech_time = chunk_time
            logger.debug(f"üéØ Speech resumed for {self.client_id}")
        
        else:
            self.silence_counter += 1
            
            # –ö–æ–º–∞–Ω–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
            if self.silence_counter >= self.silence_confirmation_chunks:
                return self._complete_command()
        
        return None
    
    def _complete_command(self) -> Optional[np.ndarray]:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –∏ –≤–æ–∑–≤—Ä–∞—Ç –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        
        if len(self.audio_buffer) == 0:
            self._reset_to_silence()
            return None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        duration = len(self.audio_buffer) / 16000  # Assuming 16kHz
        
        if duration < self.min_command_duration:
            logger.debug(f"üéØ Command too short for {self.client_id} ({duration:.1f}s)")
            self.stats['false_starts'] += 1
            self._reset_to_silence()
            return None
        
        # –£—Å–ø–µ—à–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞
        completed_audio = self.audio_buffer.copy()
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['commands_segmented'] += 1
        self.stats['successful_commands'] += 1
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if self.stats['successful_commands'] > 0:
            alpha = 0.1
            self.stats['average_command_duration'] = (
                alpha * duration + 
                (1 - alpha) * self.stats['average_command_duration']
            )
        else:
            self.stats['average_command_duration'] = duration
        
        logger.info(f"üéØ Command completed for {self.client_id}: {duration:.1f}s, "
                   f"{len(completed_audio)} samples")
        
        self._reset_to_silence()
        return completed_audio
    
    def _reset_to_silence(self):
        """–°–±—Ä–æ—Å –∫ —Å–æ—Å—Ç–æ—è–Ω–∏—é —Ç–∏—à–∏–Ω—ã"""
        self.current_state = SpeechState.SILENCE
        self.audio_buffer = np.array([])
        self.speech_counter = 0
        self.silence_counter = 0
        self.speech_start_time = None
        self.last_speech_time = None
    
    def get_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±—É—Ñ–µ—Ä–µ"""
        return {
            'client_id': self.client_id,
            'current_state': self.current_state.value,
            'buffer_size': len(self.audio_buffer),
            'stats': self.stats.copy()
        }

class ImprovedAudioProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞—É–¥–∏–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å —Ç–æ—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π"""
    
    def __init__(self, vad, asr, audio_manager):
        self.vad = vad
        self.asr = asr
        self.audio_manager = audio_manager
        
        # –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –±—É—Ñ–µ—Ä—ã
        self.client_buffers: Dict[str, ImprovedClientBuffer] = {}
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.config = {
            'segmentation_speech_threshold': 0.35,
            'segmentation_silence_threshold': 0.25,
            'min_command_duration': 0.8,
            'max_command_duration': 20.0,
            'speech_confirmation_chunks': 3,
            'silence_confirmation_chunks': 8
        }
        
        # –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.global_stats = {
            'total_clients': 0,
            'active_clients': 0,
            'total_commands_processed': 0,
            'successful_segmentations': 0,
            'false_starts_prevented': 0,
            'average_segmentation_accuracy': 100.0
        }
        
        logger.info("üéØ Improved Audio Processor initialized")
    
    def process_audio_chunk(self, client_id: str, audio_chunk: np.ndarray) -> Optional[str]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ —á–∞–Ω–∫–∞ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–∞–Ω–¥—ã –∫–æ–≥–¥–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
        """
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±—É—Ñ–µ—Ä–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
        if client_id not in self.client_buffers:
            self.client_buffers[client_id] = ImprovedClientBuffer(client_id, self.config)
            self.global_stats['total_clients'] += 1
            logger.debug(f"üéØ Created buffer for new client: {client_id}")
        
        buffer = self.client_buffers[client_id]
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ VAD score
        try:
            vad_scores = self.vad.process_chunk(audio_chunk)
            vad_score = vad_scores[0] if vad_scores else 0.0
        except Exception as e:
            logger.warning(f"VAD error for {client_id}: {e}")
            vad_score = 0.0
        
        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        completed_audio = buffer.process_chunk(audio_chunk, vad_score)
        
        if completed_audio is not None:
            # –ê—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω - –∑–∞–ø—É—Å–∫–∞–µ–º ASR
            return self._process_completed_segment(client_id, completed_audio)
        
        return None
    
    def _process_completed_segment(self, client_id: str, audio_segment: np.ndarray) -> Optional[str]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        
        try:
            self.global_stats['total_commands_processed'] += 1
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            text, confidence, processing_time = self.asr.transcribe(audio_segment)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            if text and text not in ["NO_SPEECH_DETECTED", "PROCESSING", "ASR_NOT_LOADED"]:
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ –∑–∞–ø–∏—Å–∏
                if self.audio_manager:
                    recording_path = self.audio_manager.save_audio_recording(
                        audio_segment, 
                        client_id,
                        transcription=text,
                        command_successful=True,
                        metadata={
                            'segmentation_method': 'improved_v2',
                            'confidence': confidence,
                            'processing_time': processing_time,
                            'segment_duration': len(audio_segment) / 16000
                        }
                    )
                
                self.global_stats['successful_segmentations'] += 1
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                total = self.global_stats['total_commands_processed']
                successful = self.global_stats['successful_segmentations']
                self.global_stats['average_segmentation_accuracy'] = (successful / total) * 100
                
                logger.info(f"üéØ Segmentation success for {client_id}: '{text}' "
                           f"(conf: {confidence:.3f}, {processing_time:.2f}s)")
                
                return text
            
            else:
                logger.debug(f"üéØ No valid speech in segment from {client_id}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Error processing segment from {client_id}: {e}")
            return None
    
    def cleanup_client(self, client_id: str):
        """–û—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–∞ –∫–ª–∏–µ–Ω—Ç–∞"""
        if client_id in self.client_buffers:
            del self.client_buffers[client_id]
            logger.debug(f"üéØ Cleaned up buffer for {client_id}")
    
    def get_client_info(self, client_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ"""
        if client_id in self.client_buffers:
            return self.client_buffers[client_id].get_info()
        return None
    
    def get_all_clients_info(self) -> Dict[str, Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–∞—Ö"""
        return {
            client_id: buffer.get_info() 
            for client_id, buffer in self.client_buffers.items()
        }
    
    def get_improved_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Å–µ—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
        total_commands = 0
        total_false_starts = 0
        total_successful = 0
        total_duration = 0.0
        
        for buffer in self.client_buffers.values():
            stats = buffer.stats
            total_commands += stats['commands_segmented']
            total_false_starts += stats['false_starts']
            total_successful += stats['successful_commands']
            if stats['successful_commands'] > 0:
                total_duration += stats['average_command_duration']
        
        avg_duration = total_duration / len(self.client_buffers) if self.client_buffers else 0.0
        
        return {
            **self.global_stats,
            'active_clients': len(self.client_buffers),
            'commands_segmented': total_commands,
            'segmentation_false_starts': total_false_starts,
            'segmentation_successful_commands': total_successful,
            'average_command_duration': avg_duration,
            'segmentation_mode': 'IMPROVED_V2',
            'segmentation_enabled': True
        }

def integrate_improved_segmentation(base_processor, vad, asr, audio_manager):
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º
    """
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    improved_processor = ImprovedAudioProcessor(vad, asr, audio_manager)
    
    # –ó–∞–º–µ–Ω–∞ –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–æ–≤
    base_processor.segmentation_processor = improved_processor
    
    logger.info("üéØ Improved segmentation integrated successfully")
    
    return base_processor

# –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–ª–∞—Å—Å—ã
__all__ = [
    'SpeechState',
    'ImprovedClientBuffer', 
    'ImprovedAudioProcessor',
    'integrate_improved_segmentation'
]

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
    logger.info("üéØ Testing Improved Speech Segmentation module...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
    test_config = {
        'segmentation_speech_threshold': 0.35,
        'segmentation_silence_threshold': 0.25,
        'min_command_duration': 0.8,
        'max_command_duration': 20.0,
        'speech_confirmation_chunks': 3,
        'silence_confirmation_chunks': 8
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –±—É—Ñ–µ—Ä–∞
    buffer = ImprovedClientBuffer("test_client", test_config)
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–æ–≤
    test_chunks = [
        (np.random.normal(0, 0.01, 4000), 0.1),  # –¢–∏—à–∏–Ω–∞
        (np.random.normal(0, 0.1, 4000), 0.8),   # –†–µ—á—å
        (np.random.normal(0, 0.1, 4000), 0.9),   # –†–µ—á—å
        (np.random.normal(0, 0.01, 4000), 0.1),  # –¢–∏—à–∏–Ω–∞
    ]
    
    for i, (chunk, vad_score) in enumerate(test_chunks):
        result = buffer.process_chunk(chunk, vad_score)
        logger.info(f"Chunk {i+1}: VAD={vad_score}, State={buffer.current_state.value}, "
                   f"Completed={'Yes' if result is not None else 'No'}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = buffer.get_info()
    logger.info(f"Final stats: {stats}")
    
    logger.info("‚úÖ Improved Speech Segmentation module test completed")